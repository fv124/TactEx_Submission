import requests
import json
import numpy as np
import re
import random
import time
from Tactile_LLM import get_LLM_response, structure_input_LLM

def evaluate_LLM_output(objects_structured, model_output):
    API_KEY = 'API_KEY'
    HEADERS = {
        "Authorization": f"Bearer {API_KEY}",
        "Content-Type": "application/json"
    }

    prompt = f"""
    You are an expert evaluator of scene and hardness descriptions outputed by an LLM. You will receive:

    1. A structured list of objects with positions (x,y) and hardnesses (HA) (objects_structured)
    2. A description generated by a model (model_output)

    Your task: Evaluate the description on the following criteria with a score from 0 (very bad) to 5 (very good):
    - Accuracy: Are all objects, positions, and nonzero hardness values correctly described? Is their (relative) position correct? (0-5)
    - Completeness: Are all objects mentioned? Is ripeness interpreted in the cases of lime, lemon or banana? In case of hardness 0, is
    the feedback given that the object was not present in the scene? (0-5)
    - Clarity and coherence: Is the description understandable, concise, and fluent? Does the response flow logically and consistently from start to finish? Does it make sense? (0-5)

    Context: 
    1. Positions (x,y) should be interpreted as follows: 
    x determines the front or back of the scene. The three cases are as follows:
    - if x < -170: object is positioned in the back.
    - if x is between -170 and -20: object is positioned in the center of front-back view.
    - if x > -20: object is positioned in the front.

    y determines the left or right side of the scene. The three cases are as follows:
    - if y < -520: object is positioned on the left.
    - if y is between -520 and -400: object is positioned in the center of left-right view.
    - if y > -400: object is positioned on the right side of the scene.

    2. Only bananas, limes and corns should be evaluated in terms of ripeness. For the rest of the objects, their hardness is the only 
    thing we want.

    3. In case an object has zero hardness, it means that this object could not be found. This should be communicated and if so, this is considered accurate. It should never mention in
    a prompt that an object has 0 HA, we just want to see that it could not be found.

    Examples:
    1. 
    objects_structured: [dict('object': 'lime 1', 'x': -170, 'y': -550, 'hardness': 0), dict('object': 'lemon 1', 'x': -200, 'y': -450, 'hardness': 66), dict('object': 'lime 2', 'x': -200, 'y': -430, 'hardness': 75), dict('object': 'lime 3', 'x': -200, 'y': -300, 'hardness': 73)]
    answer: In the scene, there are three limes and one lemon. Starting with the lemon, it is located towards the back and to the left with a hardness of 0 HA, which is within the ideal ripeness range for lemons. Moving on to the limes, all three are in the back. The first lime is in the center and to the left with a hardness of 68 HA, also within the ideal range. The second lime, slightly to the right of the first, has a hardness of 75 HA, and the third lime, furthest to the right, has a hardness of 73 HA. All three limes are ripe and ready to eat.
    your response: accuracy 2/5, completeness 4/5, clarity and coherence 5/5
    reason: Lime 1 was not correctly interpreted as not being recognized, so it should be two limes and one lemon + message that one lime could not be found. This should normally be communicated at end of the prompt. Also not correct that all three were thus ripe, only the second and third. Coherence and flow were good as we had brief intro and nice ending. 
    
    2.
    objects_structured: [dict('object': 'strawberry 1', 'x': -220, 'y': -450, 'hardness': 53), dict('object': 'corn 1', 'x': -200, 'y': -450, 'hardness': 80), dict('object': 'banana 1', 'x': 0, 'y': -430, 'hardness': 87), dict('object': 'banana 2', 'x': -200, 'y': -430, 'hardness': 90)]
    answer: In the scene, there are two bananas, one strawberry, and one corn. Starting with the bananas, the one in the front-center has a hardness of 87 HA, making it firmer than ideal and likely unripe. The second banana, located in the back-center, has a hardness of 90 HA, also firmer than ideal and unripe. Neither banana is ripe. That is very pity. I would not eat banana in the center. The strawberry, situated in the center-left, has a hardness of 53 HA. The corn, positioned in the back-center, has a hardness of 80 HA.
    your response: accuracy: 4/5, completeness 5/5, clarity and coherence 2/5
    reason: Strictly speaking strawberry has x = -220 and y = -450, so should be called back-center: "the strawberry, located in the center of the scene" was thus better. Hallucinations are present.

    3. 
    object_structured: [dict('object': 'strawberry 1', 'x': -180, 'y': -550, 'hardness': 85), dict('object': 'corn 1', 'x': -200, 'y': -450, 'hardness': 91), dict('object': 'banana 1', 'x': 0, 'y': -430, 'hardness': 54), dict('object': 'banana 2', 'x': -200, 'y': -430, 'hardness': 94)]
    answer: In the scene, there are two bananas, a strawberry, and a corn. The corn, positioned towards the back and center, has a hardness of 91 HA. The bananas are both in the center area: the one in the front has a hardness of 47 HA, indicating it is overripe, while the one further back has a hardness of 94 HA, suggesting it is unripe.
    your reponse: accuracy  4/5, completeness 2/5, clarity and coherence 5/5
    reason: strawberry is not communicated

    4. 
    object_structured: [dic('object': 'tomato 1', 'x': -169, 'y': -373, 'hardness': 0)] 
    answer = In the scene, a tomato is present but could not be found.
    your response: accuracy 5/5, completeness 5/5, clarity and coherence 3/5
    reason: it does not make sense to say it is present but could not be found, how would it then even know it was present? Ideally is just "it could not be found"

    Important: 
    ONLY output valid JSON. Do not include markdown, lists, explanations, or any extra text or reason. 

    Objects structured data:
    {json.dumps(objects_structured, indent=2)}

    Model description:
    {model_output}
    """

    data = {
        "model": "meta-llama/llama-4-maverick-17b-128e-instruct",
        "messages": [
            {"role": "system", "content": "You judge scene descriptions based on object data."},
            {"role": "user", "content": prompt}
        ],
        "temperature": 0.1
    }

    response = requests.post("https://api.groq.com/openai/v1/chat/completions", headers=HEADERS, json=data)
    response_text = response.json()["choices"][0]["message"]["content"]

    match = re.search(r'\{.*\}', response_text, flags=re.DOTALL)
    if match:
        scores = json.loads(match.group())
    else:
        scores = {"error": "Invalid JSON from LLM", "raw_output": response_text}

    return scores

def run_test():
    accuracy = []
    completeness = []
    clarity = []

    for j in range(10):
        print(j)
        a = random.randint(1,5)
        centroids = []
        fruit_list = []

        for i in range(a):
            y = random.randint(-640, -280)      # flexible range for left-right
            x = random.randint(-320, 130)
            tuple = (x,y)
            centroids.append(tuple)

            fruits = ['banana', 'apple', 'pear', 'peach', 'lemon', 'lime', 'tomato', 'patato', 'banana', 'banana', 'lime', 'lemon' ,'avocado', 'pepper', 'carrot', 'corn', 'zucchini', 'kiwi', 'banana', 'strawberry', 'garlic', 'potato']
            ind = random.randint(0, len(fruits)-1)
            fruit_list.append(fruits[ind])

        output = structure_input_LLM(centroids, fruit_list, None)
        try:
            response = get_LLM_response(output)

            if isinstance(response, dict) and "error" in response:
                print("API call failed:", response["error"])
                evaluation = {"accuracy": None, "completeness": None, "clarity and coherence": None}
            else:
                evaluation = evaluate_LLM_output(output, response)
                accuracy.append(evaluation["accuracy"])
                completeness.append(evaluation["completeness"])
                clarity.append(evaluation["clarity and coherence"])
                # print(output, response, evaluation)

        except Exception as e:
            print("Unexpected error during API call:", e)
            evaluation = {"accuracy": None, "completeness": None, "clarity and coherence": None}

        time.sleep(5)


    return accuracy, completeness, clarity


print(run_test())





